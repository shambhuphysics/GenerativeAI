{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f08b79db",
   "metadata": {},
   "source": [
    "## Exploring Generative AI libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667073f7",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12724185",
   "metadata": {},
   "source": [
    "##  __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#What-is-generative-AI?\">What is generative AI?</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Real-world-impact-of-generative-AI\">Real-world impact of generative AI</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#1.-Art-and-creativity\">Art and creativity</a></li>\n",
    "            <li><a href=\"#2.-Natural-language-processing-(NLP)\">Natural language processing (NLP)</a></li>\n",
    "            <li><a href=\"#3.-Computer-vision\">Computer vision</a></li>\n",
    "            <li><a href=\"#4.-Virtual-avatars\">Virtual avatars</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Text-generation-before-transformers\">Text generation before transformers</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#1.-N-gram-language-models\">N-gram language models</a></li>\n",
    "            <li><a href=\"#2.-Recurrent-neural-networks-(RNN)\">Recurrent neural networks (RNN)</a></li>\n",
    "            <li><a href=\"#3.-Long-short-term-memory-(LSTM)-and-gated-recurrent-units-(GRUs)\">Long short-term memory (LSTM) and gated recurrent units (GRUs)</a></li>\n",
    "            <li><a href=\"#4.-Seq2seq-models-with-attention\"> Seq2seq models with attention</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Transformers\">Transformers</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Implementation:-Building-a-simple-chatbot-with-transformers\">Implementation: Building a simple chatbot with transformers</a>\n",
    "                <ol>\n",
    "                    <li><a href=\"#Step-1:-Installing-libraries\">Step 1: Installing libraries</a>\n",
    "                    <li><a href=\"#Step-2:-Importing-the-required-tools-from-the-transformers-library\">\n",
    "                        Step 2: Importing the required tools from the transformers library</a>\n",
    "            </li>\n",
    "        </ol>\n",
    "    </li>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d404d9a",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430ff00",
   "metadata": {},
   "source": [
    "### Objectives \n",
    "* Gain and understanding of generative AI and its impact across various domains\n",
    "* Familarize yourself with different types of models in Generative Ai\n",
    "* Acquire the skills to build and interact with chatbot using transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1887845b",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91e8b07",
   "metadata": {},
   "source": [
    "### Generative AI\n",
    "Computer generates new and unique content based on the provided data. Generative AI is transforming multiple industries for eg.\n",
    "- 1. Art and Creativity\n",
    "     - Generative AI is employed to create stunning artworks for eg. nn augmented art, synthetic images, videos and so on\n",
    "     - Also used in music generation or augmentation\n",
    "- 2. Natural language processing (NLP)\n",
    "    - Gen AI tools like generative pre-trained transformer (GPT) have demostrated their ability to generate cohoerent and context-aware text, use to create articles, stories, marketing.\n",
    "    - Code writing\n",
    "    - Chatbots and virtual assistants: to generate human-like responses\n",
    "- 3. Computer vision (CV)\n",
    "    - Image synthesis - Models like DALL-E can generate images from textual description, useful in graphic design, advertising, and creating visual contents \n",
    "    - Deepfake Detection: with the advancement  in generative AI  techniques, the genreation of deep fake contents is also on the rise. \n",
    "\n",
    "- 4. Virtual avatars\n",
    "   - Gaming and entertainment adopts Gen AI to create avatars mimicing the human expression and emotions, bolstering user engangements in virtual\n",
    "   - Marketing: Virtual influencer, useful in digital marketing\n",
    "\n",
    "### Neural structures behind generative AI\n",
    "- Before we had the powerful transformer, which are like super-fast reader and understand lots of workds at once, there were other methods used for making computer generate text. These methods were like the building blocks that led to amazing capabilities we have today.\n",
    "\n",
    "### Large Language Models (LLMs)\n",
    "-  Large language Modelss are like supercharged brains.  They are massive computer programs with lots of neurons that learn from huge amounts of text. These models are trained to do task like understanding and generating text, and they're used in many applications. However,  there is a limitation: these modles are not very good at understanding the bigger context or the meaning of words. They work well for simple predictions but with more complex text.\n",
    "\n",
    "### Text generation before Transformers\n",
    "- 1. N-gram  language models\n",
    "  - N-grama models are like language detectives. They predict what words come next based on the words htat came before. FOr example, if you say \"The sky is,\" these models guess that the next word might be \"blue\"\n",
    "- 2. Recurrent neural network (RNN)\n",
    "  - Recurrent neural network (RNN) are specially designed to handle sequential data, making then a a powerful tool for appolications like language modeling and time series forecasting. The essence of their design lies in maintaining a 'memory' or 'hidden state' throughout the sequence by employing loops.  This enables RNNs to recognize and capture the temporal dependencies inherent in sequential data.\n",
    "  -  Hidden state: Often referred to as the network's memory, the hidden state is a dynamic storage of information about previous sequence inputs. With each new input, this hdden state is updated factoring in both the new inputs and its previous value. \n",
    "  - Temporal dependency: loops in RNNs enable information transfer across sequence steps.\n",
    "  \n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-GPXX0J87EN/%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\" width=\"60%\" height=\"60%\"> \n",
    "\n",
    "<div style=\"text-align:center\"><a href=\"https://commons.wikimedia.org/wiki/File:%E9%80%92%E5%BD%92%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E5%9B%BE.png\">Image Source</a></div>\n",
    "\n",
    "  - Illustration of RNN's operation\n",
    "    - Consider a simple sequence , such as a sentence \" I love RNNs\". The RNN interprests this sentence word by word. Begining with the word I, the RNN ingest it, generates an output, and updates its hidden state. Moving on to \"love\", the RNN processes it alongside the updated hiddgen state which already holds insights about the word \"I\". The hidden state is updated again post this.  This pattern of processing and updating continues until the last word.  By the end of the sequence, the hidden state ideally encalpsulates insights  from entire sentence.\n",
    "- 3.  Long Short-Term Memory (LSTM) and Gated Recurrent units (GRUs)\n",
    "    - Long short-term memory (LSTM) and gated recurrent units (GRUs) are advanced variations of recurrent neural network (RNNs), designed to address the limitaion of trditional RNNs and enhance their ability to model seequentila data effectively. They processed sequences one elemnt at a time, and maintained an internal state to remember past elements. While they were effective for a variety of tasks, they struggled with long sequences and long-term dependencies. \n",
    "\n",
    "### Seq2Seq models with attentions\n",
    " - Sequence-to-sequence (seq2seq) models, often bult with RNNs or LSTMs, were designed to handel task like translation where an input sequence is transformed into an output sequence. \n",
    " - The attention mechanism was introudced to allow the \"focus\" on relevant parts of the input sequence when generating the output, significatly improving performance on task like machine translation.\n",
    "\n",
    " While these methods provided significant advancements in text generation tasks, the introduction of tranformer led to a paradigm  shift. Transformers, with their self-attention mechanism, proved to be highly efficient at capturing contextual information long sequences, setting new bechmarks in various NLP tasks.\n",
    "\n",
    " ### Transformers\n",
    " Purposed in a paper titled \" Attention all you Need\" by Vaswani et al. in 2017, the transformer architecture replaced sequential processing with parallel processing. The key components behind its sucess - The attention mechanism, more precisely, self-attention.\n",
    "\n",
    "- Key steps include:\n",
    "  - Tokenization: The first step is breaking down a sentence into tokens (words or subwords)\n",
    "  - Embedding:  Each token is represented as a vector, capturing its meaning.\n",
    "  - Self-attention:  The model computes scores determining the importance of every other word for a particular word in the sequences. These socres are used to weight the input toakens and produce a new representation of the sequence. For instance, in the sentence \" He gave her a gift , because she'd helped him\", understanding who \"her\", referes to requires the model to pay attention to ther words in the sentence. The transformer does this for every word, considering the entire context, which particulary powerful for understanding meaning.\n",
    " - Feed-forward neural networks: After attention, each position is passed through a feed-forward network seperately.\n",
    " - Output sequence: The model produces an ouput sequence, which can be used for various tasks, like classification, translation, or text generation\n",
    " -Layering: Importantly, the transformers are deep models with multiple layers of attention and feed-forward networks, allowing them to learn complex patterns. \n",
    " \n",
    "- The architecture's flexibility has allowed transformers to be used beyond NLP, finding applications in image and video processing too. In NLP, transformer-based models like BERT , GPT,  and their varients have set state-of-the art resultsin various tasks, from text classificaitons to translations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d027ecb9",
   "metadata": {},
   "source": [
    "### Building a simple chatbot with tranformers\n",
    "- We will use tranformer library from Hugging Face - a very useful NLP toolkit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27da5ac",
   "metadata": {},
   "source": [
    "### Step 1: Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f452a624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (4.57.0)\n",
      "Requirement already satisfied: filelock in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.13.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->transformers) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: sentencepiece in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (4.13.0)\n",
      "Requirement already satisfied: setuptools in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (75.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch) (3.4.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torchtext in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (0.18.0)\n",
      "Requirement already satisfied: tqdm in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torchtext) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torchtext) (2.32.3)\n",
      "Requirement already satisfied: torch>=2.3.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torchtext) (2.8.0)\n",
      "Requirement already satisfied: numpy in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torchtext) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (4.13.0)\n",
      "Requirement already satisfied: setuptools in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (75.6.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (1.13.1.3)\n",
      "Requirement already satisfied: triton==3.4.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch>=2.3.0->torchtext) (3.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->torchtext) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->torchtext) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->torchtext) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->torchtext) (2025.8.3)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=2.3.0->torchtext) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from jinja2->torch>=2.3.0->torchtext) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -qq tensorflow\n",
    "!pip install transformers \n",
    "!pip install sentencepiece\n",
    "!pip install torch\n",
    "!pip install torchtext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc1442",
   "metadata": {},
   "source": [
    "### Step 2: Importing the required tools from Transformer library\n",
    "- model is an instance of the class \" AutoModelForSeq2SeqLM \". This class lets you interact with your chosen language model.\n",
    "- tokenizer is an instance of the class \"AutoTokenizer\". THis class streamlines your input and presents it to the language modelin the most efficent manner.  It achieves this by converting your text input into tokens,  which is models prefered way of interpreting text. \n",
    "- We choose \"facebook/blenderbot-400M-distill\" for this example model because it is freely available under and open-source licence and operates at a relatively brisk pace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9be01af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-05 15:39:58.270059: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-10-05 15:39:58.334647: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-10-05 15:40:02.517926: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "# importing the required classes\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "# select the facebook/blenderbot-400M-distill model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"facebook/blenderbot-400M-distill\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/blenderbot-400M-distill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72db21d0",
   "metadata": {},
   "source": [
    "- Since the model is fully loaded , let us set up the chat function to eable real-time interaction with the chatbot using facebook blenderbot pretrained model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e124ae3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 714,  315, 3938,  306, 3830, 2453,    2]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer.encode(\"What is capital of China ?\", return_tensors=\"pt\")\n",
    "inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7bae6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(inputs, max_new_tokens=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1ba1b74f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   1,  384, 3938,  315, 1539, 3572,  278,   21,  228,  452,  315,  271,\n",
       "         685, 1402,  461,  554, 2224,  302, 3830,   21,    2])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8bf95ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s> The capital is Beijing.  It is the most populous city in China.</s>'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ee3bc72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital is Beijing.  It is the most populous city in China.'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(output[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19601214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You: Hi\n",
      "your name is None\n"
     ]
    }
   ],
   "source": [
    "name = print(f\"You: {input('What is your name ? ')}\")\n",
    "print(f'your name is {name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bdfe9f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi, do you like hiking? I love hiking, especially in the mountains.\n",
      "Chatbot: What do you think about it? Do you think it's a good idea to do it?\n",
      "Chatbot: What do you mean it isn't? I'm not sure what you mean by that.\n",
      "Chatbot: What do you think about it? Do you think it's a good idea to do it?\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# Define the chat function\n",
    "def chat_with_bot():\n",
    "    while True:\n",
    "        # Get user input\n",
    "        input_text = input(\"You: \")\n",
    "\n",
    "        # Exit conditions\n",
    "        if input_text.lower() in [\"quit\", \"exit\", \"bye\"]:\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "\n",
    "        # Tokenize input and generate response\n",
    "        inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "        outputs = model.generate(inputs, max_new_tokens=150) \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "\n",
    "        # Display bot's response\n",
    "        print(\"Chatbot:\", response)\n",
    "\n",
    "# Start chatting\n",
    "chat_with_bot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "655c3e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_fb = \"facebook/blenderbot-400M-distill\"\n",
    "model_gl = \"google/flan-t5-base\"\n",
    "model_gl_sml = \"google/flan-t5-small\"\n",
    "model_fb_bart = \"facebook/bart-base\"\n",
    "\n",
    "model_llama = \"meta/llama-3.1\"\n",
    "model_gemma = \"google/gemma-2\"\n",
    "model_vicuna = \"vicuna/vicuna-13b\"\n",
    "model_falcon = \"tiiuae/falcon\"\n",
    "model_stablelm = \"stabilityai/stablelm\"\n",
    "model_mistral = \"mistral/mistral-7b\"\n",
    "model_openorca = \"openorca/platypus2-13b\"\n",
    "model_gpt_neox = \"eleutherai/gpt-neox-20b\"\n",
    "model_bloom = \"bigscience/bloom\"\n",
    "model_alpaca = \"chavinlo/alpaca-native-7b\"\n",
    "model_tulu = \"tulu/tulu-3-405b\"\n",
    "\n",
    "def chat_with_bot(mdl_type):\n",
    "  from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "  model =  AutoModelForSeq2SeqLM.from_pretrained(mdl_type)\n",
    "  tokenizer = AutoTokenizer.from_pretrained(mdl_type)\n",
    "  while True:\n",
    "    # Get user inputs\n",
    "    input_text = input(\"You: \")\n",
    "    # Set exit conditions\n",
    "    if input_text in ['exit', 'quit', 'bye', '']:\n",
    "      print(f'Goodbye !')\n",
    "      break\n",
    "      \n",
    "    else:\n",
    "      #first tokenize the input text\n",
    "      inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "      # feed that tokens to model to generate outputs\n",
    "      ouputs = model.generate(inputs, max_new_tokens=150)\n",
    "      # decode the ouputs from model into text\n",
    "      response = tokenizer.decode(ouputs[0], skip_special_tokens=True)\n",
    "      # print the response\n",
    "      print('Chatbot:', response)\n",
    "\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f541df44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: The man who was a snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby snobby\n",
      "Goodbye !\n"
     ]
    }
   ],
   "source": [
    "chat_with_bot(mdl_type=model_gl_sml)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matinfo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
