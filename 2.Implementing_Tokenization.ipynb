{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ab1b37",
   "metadata": {},
   "source": [
    "[Open 2.Implementing_Tokenization.ipynb in Google Colab](https://colab.research.google.com/github/shambhuphysics/GenerativeAI/blob/main/2.Implementing_Tokenization.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dff43a",
   "metadata": {},
   "source": [
    "# Implementing Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979efd43",
   "metadata": {},
   "source": [
    "- Tokenizers are essential tools in natural language processing (NLP) that break down text into smaller units called token. These tokens can be words, characters, or subwords, making complex text  understandable to computers\n",
    "- By dividing text into manageable pieces, tokenizers enable machines to process and analyze human language, powering various language related applications:\n",
    "  - translation\n",
    "  - sentiment analysis\n",
    "  - chatbots and so on\n",
    "\n",
    "- Tokenizers bridge the gap between human language and machine understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d626726e",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\">\n",
    "  <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0205EN-SkillsNetwork/tokenizer.png\" width=\"700px\" alt=\"wizard\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8101ea42",
   "metadata": {},
   "source": [
    "# __Table of Contents__\n",
    "\n",
    "<ol>\n",
    "    <li><a href=\"#Objectives\">Objectives</a></li>\n",
    "    <li>\n",
    "        <a href=\"#Setup\">Setup</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Installing-required-libraries\">Installing required libraries</a></li>\n",
    "            <li><a href=\"#Importing-required-libraries\">Importing required libraries</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#What-is-a-tokenizer-and-why-do-we-use-it?\">What is a tokenizer and why do we use it?</a>\n",
    "    </li>\n",
    "    <li><a href=\"#Types-of-tokenizer\">Types of tokenizer</a></li>\n",
    "        <ol>\n",
    "            <li><a href=\"#Word-based-tokenizer\">Word-based tokenizer</a></li>\n",
    "            <li><a href=\"#Character-based-tokenizer\">Character-based tokenizer</a></li>\n",
    "            <li><a href=\"#Subword-based-tokenizer\">Subword-based tokenizer</a></li>\n",
    "                <ol>\n",
    "                    <li><a href=\"#WordPiece\">WordPiece</a></li>\n",
    "                    <li><a href=\"#Unigram-and-SentencePiece\">Unigram and SentencePiece</a></li>\n",
    "                </ol>\n",
    "        </ol>\n",
    "    <li>\n",
    "        <a href=\"#Tokenization-with-PyTorch\">Tokenization with PyTorch</a>\n",
    "    </li>\n",
    "    <li>\n",
    "        <a href=\"#Token-indices\">Token indices</a>\n",
    "        <ol>\n",
    "            <li><a href=\"#Out-of-vocabulary-(OOV)\">Out-of-vocabulary (OOV)</a></li>\n",
    "        </ol>\n",
    "    </li>\n",
    "    <li><a href=\"#Exercise:-Comparative-text-tokenization-and-performance-analysis\">Exercise: Comparative text tokenization and performance analysis</a></li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49afd02",
   "metadata": {},
   "source": [
    "# Objectivies \n",
    "\n",
    "- Understand the concept of tokenization and its importance in natural language processing\n",
    "- Identify and explain `word-based`, `character-based`, `subwords-based` tokenization methods.\n",
    "- Apply tokenization strategies to preprocess raw textual data before using it in machine learing models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6742175",
   "metadata": {},
   "source": [
    "# Libraries Required:\n",
    "1. Natural language toolkit(nltk)\n",
    " - NLTK is set of tools and resources, used for data managment such as text preprocessing and analysis\n",
    "2. Spacy\n",
    "-  Preprocessing text\n",
    "3. BertTokenizer\n",
    "  - A Hugging Face Transformers library; used for tokenizing text according to `BERT` models.\n",
    "4. XLNetTokenizere\n",
    "  - A Hugging Face Transformer library,  used for tokenizing text according to `XLNet` Models.\n",
    "5. Torchtext\n",
    "  - A pytorch library useful for text data preprocessing, tokenization, vocabulary managments and batching.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ae64a",
   "metadata": {},
   "source": [
    "# Installing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d3f5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (3.9.2)\n",
      "Requirement already satisfied: click in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from nltk) (2025.9.18)\n",
      "Requirement already satisfied: tqdm in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: transformers==4.42.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (4.42.1)\n",
      "Requirement already satisfied: filelock in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (0.35.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (1.26.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (2025.9.18)\n",
      "Requirement already satisfied: requests in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from transformers==4.42.1) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (4.13.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.42.1) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->transformers==4.42.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->transformers==4.42.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->transformers==4.42.1) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->transformers==4.42.1) (2025.8.3)\n",
      "Requirement already satisfied: sentencepiece in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (0.2.1)\n",
      "Requirement already satisfied: spacy in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (3.8.7)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (1.0.13)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (8.3.6)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (1.26.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (2.10.3)\n",
      "Requirement already satisfied: jinja2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (75.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.13.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.8.3)\n",
      "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.22.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Requirement already satisfied: wrapt in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
      "Using cached numpy-2.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyiron-base 0.10.9 requires numpy<=1.26.4,>=1.23.5, but you have numpy 2.3.3 which is incompatible.\n",
      "pyiron-base 0.10.9 requires sqlalchemy<=2.0.36,>=2.0.22, but you have sqlalchemy 2.0.40 which is incompatible.\n",
      "transformers 4.42.1 requires numpy<2.0,>=1.17, but you have numpy 2.3.3 which is incompatible.\n",
      "pyfileindex 0.0.31 requires numpy<=2.1.2,>=1.23.5, but you have numpy 2.3.3 which is incompatible.\n",
      "mendeleev 0.19.0 requires numpy<2.0,>=1.26; python_version >= \"3.12\", but you have numpy 2.3.3 which is incompatible.\n",
      "pyiron-atomistics 0.6.18 requires numpy<=1.26.4,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\n",
      "pyiron-atomistics 0.6.18 requires scikit-learn<=1.6.0,>=1.2.1, but you have scikit-learn 1.7.1 which is incompatible.\n",
      "pyprocar 6.5.0 requires numpy<2.0, but you have numpy 2.3.3 which is incompatible.\n",
      "monty 2024.10.21 requires numpy<2.0.0, but you have numpy 2.3.3 which is incompatible.\n",
      "scipy 1.14.1 requires numpy<2.3,>=1.23.5, but you have numpy 2.3.3 which is incompatible.\n",
      "structuretoolkit 0.0.28 requires numpy<=1.26.4,>=1.23.5, but you have numpy 2.3.3 which is incompatible.\n",
      "emmet-core 0.84.4 requires numpy<2, but you have numpy 2.3.3 which is incompatible.\n",
      "h5io-browser 0.1.4 requires numpy<=2.1.2,>=1.23.5, but you have numpy 2.3.3 which is incompatible.\n",
      "atomistics 0.1.32 requires numpy<=1.26.4,>=1.23.5, but you have numpy 2.3.3 which is incompatible.\n",
      "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 2.3.3 which is incompatible.\n",
      "pylammpsmpi 0.2.26 requires numpy<=1.26.4,>=1.23.5, but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.3.3\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m58.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.3.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/thinc/types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/thinc/compat.py\", line 35, in <module>\n",
      "    import torch\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/lustre/home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "Collecting de-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Requirement already satisfied: scikit-learn in /home/ucfbsbh/.local/lib/python3.12/site-packages (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from scikit-learn) (2.3.3)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.3.3\n",
      "    Uninstalling numpy-2.3.3:\n",
      "      Successfully uninstalled numpy-2.3.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyiron-base 0.10.9 requires numpy<=1.26.4,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\n",
      "pyiron-base 0.10.9 requires sqlalchemy<=2.0.36,>=2.0.22, but you have sqlalchemy 2.0.40 which is incompatible.\n",
      "transformers 4.42.1 requires numpy<2.0,>=1.17, but you have numpy 2.2.6 which is incompatible.\n",
      "pyfileindex 0.0.31 requires numpy<=2.1.2,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\n",
      "mendeleev 0.19.0 requires numpy<2.0,>=1.26; python_version >= \"3.12\", but you have numpy 2.2.6 which is incompatible.\n",
      "pyiron-atomistics 0.6.18 requires numpy<=1.26.4,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
      "pyiron-atomistics 0.6.18 requires scikit-learn<=1.6.0,>=1.2.1, but you have scikit-learn 1.7.1 which is incompatible.\n",
      "pyprocar 6.5.0 requires numpy<2.0, but you have numpy 2.2.6 which is incompatible.\n",
      "monty 2024.10.21 requires numpy<2.0.0, but you have numpy 2.2.6 which is incompatible.\n",
      "structuretoolkit 0.0.28 requires numpy<=1.26.4,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\n",
      "emmet-core 0.84.4 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "h5io-browser 0.1.4 requires numpy<=2.1.2,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\n",
      "atomistics 0.1.32 requires numpy<=1.26.4,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\n",
      "pylammpsmpi 0.2.26 requires numpy<=1.26.4,>=1.23.5, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\n",
      "Requirement already satisfied: torch==2.2.2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: filelock in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (4.13.0)\n",
      "Requirement already satisfied: sympy in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2) (12.8.93)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from jinja2->torch==2.2.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from sympy->torch==2.2.2) (1.3.0)\n",
      "Requirement already satisfied: torchtext==0.17.2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (0.17.2)\n",
      "Requirement already satisfied: tqdm in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torchtext==0.17.2) (4.67.1)\n",
      "Requirement already satisfied: requests in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torchtext==0.17.2) (2.32.3)\n",
      "Requirement already satisfied: torch==2.2.2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torchtext==0.17.2) (2.2.2)\n",
      "Requirement already satisfied: numpy in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torchtext==0.17.2) (2.2.6)\n",
      "Requirement already satisfied: filelock in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (4.13.0)\n",
      "Requirement already satisfied: sympy in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (1.13.3)\n",
      "Requirement already satisfied: networkx in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (2025.5.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from torch==2.2.2->torchtext==0.17.2) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2->torchtext==0.17.2) (12.8.93)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->torchtext==0.17.2) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->torchtext==0.17.2) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->torchtext==0.17.2) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from requests->torchtext==0.17.2) (2025.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from jinja2->torch==2.2.2->torchtext==0.17.2) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ucfbsbh/mambaforge/envs/matinfo/lib/python3.12/site-packages (from sympy->torch==2.2.2->torchtext==0.17.2) (1.3.0)\n",
      "Collecting numpy==1.26.0\n",
      "  Using cached numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
      "Using cached numpy-1.26.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.9 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pyiron-contrib 0.1.18 requires numpy>=1.26.4, but you have numpy 1.26.0 which is incompatible.\n",
      "pyiron-base 0.10.9 requires sqlalchemy<=2.0.36,>=2.0.22, but you have sqlalchemy 2.0.40 which is incompatible.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\n",
      "pyiron-atomistics 0.6.18 requires scikit-learn<=1.6.0,>=1.2.1, but you have scikit-learn 1.7.1 which is incompatible.\n",
      "kinisi 1.1.1 requires numpy>=1.26.4, but you have numpy 1.26.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers==4.42.1\n",
    "!pip install sentencepiece\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download de_core_news_sm\n",
    "!pip install scikit-learn\n",
    "!pip install torch==2.2.2\n",
    "!pip install torchtext==0.17.2\n",
    "!pip install numpy==1.26.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7fbfc",
   "metadata": {},
   "source": [
    "# Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c022b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/ucfbsbh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/ucfbsbh/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fb4f18094ba40cdb29005abe19a308a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.util import ngrams\n",
    "from transformers import BertTokenizer\n",
    "from transformers import XLNetTokenizer\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac609717",
   "metadata": {},
   "source": [
    "# 1. Tokenizers:\n",
    "- Tokenizers play a pivotal role in NLP, sgementing text into smaller units known as tokens. These tokens are subsequently transformed into numerical representation called token indices, which are directly employed by deep learning algorithms.\n",
    "\n",
    "  - Text (I Love You) ---> Tokenization ---> Tokens ['I', 'Love', 'You] \n",
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%201.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>\n",
    "\n",
    "## 1.1 Types of Tokenizers:\n",
    "The meaningful representation may vary depending on the model in use. Various models employ distinct tokenization algorithms. Transforming text into numerical value might appear straightforward initially but it encompasses several consideration that must be kept in mind. \n",
    "\n",
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%202.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c903374e",
   "metadata": {},
   "source": [
    "### 1.1.1  Word-Based tokenizers\n",
    "- A. Using  `nltk's` `word_tokenize`:\n",
    "  - The text is splitting into token based on the words. There are several rules for word-based tokenizers such as spliting on spaces, spliting on puntuations, and so on.  Each option assigns a specific ID to the split word.  For eg. lets use `nltk's` `word_tokenize`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "287f4e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'is', 'a', 'sample', 'sentences', 'for', 'word', 'tokenization', '.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"This is a sample sentences for word tokenization.\"\n",
    "\n",
    "token = word_tokenize(text)\n",
    "\n",
    "token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac94e5e",
   "metadata": {},
   "source": [
    "- Note: General libraries like nltk and spaCy often splits word like `don't` and `couldn't` , which are contractions, into different indivisual words. There are no universal rule, and each library has it's own tokenization rule for word-based tokenizers. \n",
    "    - The general guideline is to preserve the input format after tokenization to mach how the model was tranined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd1814e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'could', \"n't\", 'help', 'the', 'dog', '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I couldn't help the dog.\"\n",
    "\n",
    "token = word_tokenize(text)\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9305f5bb",
   "metadata": {},
   "source": [
    "- B. Using  `spaCy's & torchtext's` `word_tokenize`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9b79253",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'could', \"n't\", 'help', 'the', 'dog', '.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I couldn't help the dog.\"\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "token = [ token.text for token in doc]\n",
    "\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3fe4db6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I PRON nsubj\n",
      "could AUX aux\n",
      "n't PART neg\n",
      "help VERB ROOT\n",
      "the DET det\n",
      "dog NOUN dobj\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "# this spaCy and torchtext tokenization funtion also shows more details of token \n",
    "\n",
    "for token in doc:\n",
    "  print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691ba412",
   "metadata": {},
   "source": [
    "- Please note that:\n",
    "  - I PRON nsubj: \"I\" is a pronoun (PRON), and is the nominal subject (nsub) of the sentences.\n",
    "  - help VERB ROOT: \"help\" is a verb(VERB) and is the root action (ROOT) of the sentences.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a722a86",
   "metadata": {},
   "source": [
    "The problem with this algorithm is that words with the similar meaning will be assigned different IDs, resulting in them being treated as entirely separate words with distinct meaning.  For example Unicorns word is the plural from of Unicorn, but a word-based tokenizer would tokenize them as two seperate words, potentially causing the model to miss their sematic meaning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "181df45e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unicorns', 'are', 'real', ',', 'I', 'saw', 'a', 'unicorn', 'yesterday']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text= \"Unicorns are real, I saw a unicorn yesterday\"\n",
    "\n",
    "token = word_tokenize(text)\n",
    "token "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be791f75",
   "metadata": {},
   "source": [
    "Each word splits into a token , leading to a significant increase in model's overall vocabulary. Each token is mapped to a large vector containing the word's meaning, resultsing in large model parameters. \n",
    "Language generally have a large number of words, the vocabularies based on them will always be extensive. However, the number of characters in laguage is always fewer compared to the number of words. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568a3bff",
   "metadata": {},
   "source": [
    "## 1.1.2 Character Based tokenizers\n",
    "As the nmae suggest, character-based tokenization involves splittting text into indivisual characters. THe advantages of using this approaach is that resulting vocabulary are inherently samll. Furthermore, since language have a limited set of characters, the numbers of out-of-vocabulary tokens is also limited, reducing token wastage.\n",
    "\n",
    "For example, input text: This is a sample sentence for tokenization. \n",
    "\n",
    "Character-based tokenization outputs ['T', 'h', 'i', 's', 'i', 's', 'a', 'm', 'p'...]\n",
    "\n",
    "However, it is important  to note that the characte-based tokenization has its limitations. Single characters may not convey the information as entire words, and the overall token length increases significantly, potentially causing issues with model size and loss of performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0f86b5",
   "metadata": {},
   "source": [
    "## 1.1.3 Subword-based Tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24593c2",
   "metadata": {},
   "source": [
    "The subword-based tokenizer allows frequently used words to remain unsplit while breaking down infrequent words into meaninful subwords. \n",
    "- SentencePiece, or WordPiece are commonly used for subword tokenization.\n",
    "- These methods learn subwords units from a given text corpus, identifying common prefixes, suffixes, and root words as subwords token based on their frequency of occurance. \n",
    "- This helps preserving the semantic information assocites  with the overall word. \n",
    " some example is shown below "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47ac865",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0201EN-Coursera/images/Tokenization%20lab%20Diagram%203.png\" width=\"50%\" alt=\"Image Description\">\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75443a6d",
   "metadata": {},
   "source": [
    "### A. WordPiece\n",
    "- Initially, WordPiece initializes its vocabulary to include every character presents in the traning data and progressively learn s a specified number of merged rules. \n",
    "\n",
    "- WordPiece doesn't select the most frequent symbol pair but rather the one that maximizes the likelihood of the traning data when added to the vocabulary. \n",
    "\n",
    "- WordPiece evaluates what it sacrifies by merging two symbols to ensure it's worthwhile enddeavor. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4be80e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ibm', 'taught', 'me', 'token', '##ization']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using BertTokenizer as WordPiece tokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization\")\n",
    "##ization  indicates that it is a part of a orginal word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9661d7",
   "metadata": {},
   "source": [
    "### B. Unigram and SentencePiece\n",
    "\n",
    "- Unigram starts with a large list of possible words and gradually narrowing it down based on frequency of those text\n",
    "\n",
    "- Sentencepiece produces token , assigns IDs, maintaing the cosistency. \n",
    "\n",
    "- Unigram+SentencePiece work together, unigram reduces the vocabulary efficenlty while SentencePiece handles subword segmentaiton and IDs assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6adfc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f485cf2ca0442a3a98a21c45677d532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5488f1d8a9f84b41bbaee044e2855215",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.38M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9173d76ea7471d88188a7681555ac1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['▁IBM', '▁taught', '▁me', '▁token', 'ization']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "tokenizer.tokenize(\"IBM taught me tokenization\")\n",
    "# _ <-- refred as whitespace, indicates as whole word\n",
    "# word without _ , indicates subword token\n",
    "# '.' <--- puntuation is treated seperately "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d276ff",
   "metadata": {},
   "source": [
    "### C. Tokenization with Pytorch\n",
    "- Pytorch's torchtext libary breaks down text into token (words or subwords), facilating into numberial format, assigning unique integers allowing them to feed into NN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21c6eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['introduction', 'to', 'nlp']\n",
      "['basics', 'of', 'pytorch']\n",
      "['nlp', 'techniques', 'for', 'text', 'classification']\n",
      "['named', 'entity', 'recognition', 'with', 'pytorch']\n",
      "['sentiment', 'analysis', 'using', 'pytorch']\n",
      "['machine', 'translation', 'with', 'pytorch']\n",
      "['nlp', 'named', 'entity', ',', 'sentiment', 'analysis', ',', 'machine', 'translation']\n",
      "['machine', 'translation', 'with', 'nlp']\n",
      "['named', 'entity', 'vs', 'sentiment', 'analysis', 'nlp']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "dataset = [\n",
    "    (1,\"Introduction to NLP\"),\n",
    "    (2,\"Basics of PyTorch\"),\n",
    "    (1,\"NLP Techniques for Text Classification\"),\n",
    "    \n",
    "    (3,\"Named Entity Recognition with PyTorch\"),\n",
    "    (3,\"Sentiment Analysis using PyTorch\"),\n",
    "    (3,\"Machine Translation with PyTorch\"),\n",
    "    (1,\" NLP Named Entity,Sentiment Analysis,Machine Translation \"),\n",
    "    (1,\" Machine Translation with NLP \"),\n",
    "    (1,\" Named Entity vs Sentiment Analysis  NLP \")]\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "for idx,sentence in enumerate(dataset):\n",
    "  print(tokenizer(dataset[idx][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8dbdc",
   "metadata": {},
   "source": [
    "- Token Indices\n",
    "  - to represent word as number, `build_vocab_from_iterator`\n",
    "  - dataset is iterable therefore, generator funtion is used to get one at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ab37526",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yield_tokens(data_iter):\n",
    "  for _, text in data_iter:\n",
    "    yield tokenizer(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b925eeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_iterator = yield_tokens(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be7666b",
   "metadata": {},
   "source": [
    "- Tokenization can produce words not present in the vocabulary due to rarity or absence during vocabulary building.  \n",
    "- Out-of-vocabulary (OOV) words encountered in tasks like text generation or language modeling are represented using the `<unk>` token.  \n",
    "- Example: \"apple\" in the vocabulary is used normally, while \"pineapple\" (OOV) is replaced with `<unk>`.  \n",
    "- Including `<unk>` in the vocabulary ensures a consistent method for handling OOV words in NLP tasks.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0178bfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<unk>', 'nlp', 'pytorch', 'analysis', 'entity', 'machine', 'named', 'sentiment', 'translation', 'with', ',', 'basics', 'classification', 'for', 'introduction', 'of', 'recognition', 'techniques', 'text', 'to', 'using', 'vs']\n",
      "{'vs': 21, 'to': 19, 'of': 15, 'introduction': 14, 'recognition': 16, 'for': 13, 'nlp': 1, 'entity': 4, 'pytorch': 2, 'translation': 8, 'techniques': 17, 'machine': 5, 'named': 6, ',': 10, 'text': 18, 'sentiment': 7, '<unk>': 0, 'with': 9, 'basics': 11, 'using': 20, 'analysis': 3, 'classification': 12}\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(yield_tokens(dataset), specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "print(vocab.get_itos() ) # vocabulary is built \n",
    "print(vocab.get_stoi())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4bbe43ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentences: ['named', 'entity', 'recognition', 'with', 'pytorch']\n",
      "Token Indices: [6, 4, 16, 9, 2]\n"
     ]
    }
   ],
   "source": [
    "def get_tokenized_sentence_and_indicess(iterator):\n",
    "  tokenized_sentence = next(iterator)\n",
    "  token_indices = [vocab[token] for token in tokenized_sentence]\n",
    "  return tokenized_sentence, token_indices \n",
    "\n",
    "tokenized_sentence, token_indices = get_tokenized_sentence_and_indicess(my_iterator)\n",
    "\n",
    "print(\"Tokenized Sentences:\", tokenized_sentence)\n",
    "print(\"Token Indices:\", token_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "99aec470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lines after adding special tokens:\n",
      " [['<bos>', 'IBM', 'taught', 'me', 'tokenization', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>'], ['<bos>', 'Special', 'tokenizers', 'are', 'ready', 'and', 'they', 'will', 'blow', 'your', 'mind', '<eos>'], ['<bos>', 'just', 'saying', 'hi', '!', '<eos>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']]\n"
     ]
    }
   ],
   "source": [
    "lines = [\"IBM taught me tokenization\", \n",
    "         \"Special tokenizers are ready and they will blow your mind\", \n",
    "         \"just saying hi!\"]\n",
    "\n",
    "special_symbols = ['unk', '<pad>', '<bos>', '<eos>']\n",
    "\n",
    "tokenizer_en = get_tokenizer('spacy', language=\"en_core_web_sm\")\n",
    "\n",
    "tokens = []\n",
    "\n",
    "max_length = 0\n",
    "\n",
    "for line in lines:\n",
    "  tokenized_line = tokenizer_en(line)\n",
    "  tokenized_line = ['<bos>'] + tokenized_line + ['<eos>']\n",
    "  tokens.append(tokenized_line)\n",
    "  max_length = max(max_length, len(tokenized_line))\n",
    "  \n",
    "for i in range(len(tokens)):\n",
    "  tokens[i] = tokens[i] + ['<pad>'] * (max_length - len(tokens[i]))\n",
    "\n",
    "print(\"lines after adding special tokens:\\n\", tokens)\n",
    "\n",
    "  \n",
    "vocab = build_vocab_from_iterator(tokens, specials = ['<unk>'])\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "acd2e1aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<unk>',\n",
       " '<pad>',\n",
       " '<bos>',\n",
       " '<eos>',\n",
       " '!',\n",
       " 'IBM',\n",
       " 'Special',\n",
       " 'and',\n",
       " 'are',\n",
       " 'blow',\n",
       " 'hi',\n",
       " 'just',\n",
       " 'me',\n",
       " 'mind',\n",
       " 'ready',\n",
       " 'saying',\n",
       " 'taught',\n",
       " 'they',\n",
       " 'tokenization',\n",
       " 'tokenizers',\n",
       " 'will',\n",
       " 'your']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_itos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a7eb26b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'will': 20,\n",
       " 'tokenizers': 19,\n",
       " 'tokenization': 18,\n",
       " 'taught': 16,\n",
       " 'your': 21,\n",
       " 'saying': 15,\n",
       " '<unk>': 0,\n",
       " 'and': 7,\n",
       " 'hi': 10,\n",
       " '<pad>': 1,\n",
       " '<bos>': 2,\n",
       " 'they': 17,\n",
       " '<eos>': 3,\n",
       " '!': 4,\n",
       " 'ready': 14,\n",
       " 'IBM': 5,\n",
       " 'are': 8,\n",
       " 'Special': 6,\n",
       " 'mind': 13,\n",
       " 'me': 12,\n",
       " 'blow': 9,\n",
       " 'just': 11}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.get_stoi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7301202a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Token Frequencies: {'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Going through the world of tokenization has been like walking through a huge maze made of words, symbols, and meanings. Each turn shows a bit more about the cool ways computers learn to understand our language. And while I'm still finding my way through it, the journey’s been enlightening and, honestly, a bunch of fun.\n",
    "Eager to see where this learning path takes me next!\"\n",
    "\"\"\"\n",
    "\n",
    "# Counting and displaying tokens and their frequency\n",
    "from collections import Counter\n",
    "def show_frequencies(tokens, method_name):\n",
    "    print(f\"{method_name} Token Frequencies: {dict(Counter(tokens))}\\n\")\n",
    "\n",
    "from datetime import datetime \n",
    "\n",
    "#NLTK Tokenization\n",
    "start_time =  datetime.now()\n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "nltk_time = datetime.now() - start_time\n",
    "nltk_time\n",
    "show_frequencies(nltk_tokens, \"NLTK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f76d6c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Tokens: ['Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', \"''\"]\n",
      "Time Taken: 0:00:00.000389 seconds\n",
      "\n",
      "NLTK Token Frequencies: {'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, \"''\": 1}\n",
      "\n",
      "SpaCy Tokens: ['\\n', 'Going', 'through', 'the', 'world', 'of', 'tokenization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'Each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'And', 'while', 'I', \"'m\", 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’s', 'been', 'enlightening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', '\\n', 'Eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"', '\\n']\n",
      "Time Taken: 0:00:02.522066 seconds\n",
      "\n",
      "SpaCy Token Frequencies: {'\\n': 3, 'Going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'tokenization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 2, 'meanings': 1, '.': 3, 'Each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'And': 1, 'while': 1, 'I': 1, \"'m\": 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’s': 1, 'enlightening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'Eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
      "\n",
      "Bert Tokens: ['going', 'through', 'the', 'world', 'of', 'token', '##ization', 'has', 'been', 'like', 'walking', 'through', 'a', 'huge', 'maze', 'made', 'of', 'words', ',', 'symbols', ',', 'and', 'meanings', '.', 'each', 'turn', 'shows', 'a', 'bit', 'more', 'about', 'the', 'cool', 'ways', 'computers', 'learn', 'to', 'understand', 'our', 'language', '.', 'and', 'while', 'i', \"'\", 'm', 'still', 'finding', 'my', 'way', 'through', 'it', ',', 'the', 'journey', '’', 's', 'been', 'en', '##light', '##ening', 'and', ',', 'honestly', ',', 'a', 'bunch', 'of', 'fun', '.', 'eager', 'to', 'see', 'where', 'this', 'learning', 'path', 'takes', 'me', 'next', '!', '\"']\n",
      "Time Taken: 0:00:00.000866 seconds\n",
      "\n",
      "Bert Token Frequencies: {'going': 1, 'through': 3, 'the': 3, 'world': 1, 'of': 3, 'token': 1, '##ization': 1, 'has': 1, 'been': 2, 'like': 1, 'walking': 1, 'a': 3, 'huge': 1, 'maze': 1, 'made': 1, 'words': 1, ',': 5, 'symbols': 1, 'and': 3, 'meanings': 1, '.': 3, 'each': 1, 'turn': 1, 'shows': 1, 'bit': 1, 'more': 1, 'about': 1, 'cool': 1, 'ways': 1, 'computers': 1, 'learn': 1, 'to': 2, 'understand': 1, 'our': 1, 'language': 1, 'while': 1, 'i': 1, \"'\": 1, 'm': 1, 'still': 1, 'finding': 1, 'my': 1, 'way': 1, 'it': 1, 'journey': 1, '’': 1, 's': 1, 'en': 1, '##light': 1, '##ening': 1, 'honestly': 1, 'bunch': 1, 'fun': 1, 'eager': 1, 'see': 1, 'where': 1, 'this': 1, 'learning': 1, 'path': 1, 'takes': 1, 'me': 1, 'next': 1, '!': 1, '\"': 1}\n",
      "\n",
      "XLNet Tokens: ['▁Going', '▁through', '▁the', '▁world', '▁of', '▁token', 'ization', '▁has', '▁been', '▁like', '▁walking', '▁through', '▁a', '▁huge', '▁maze', '▁made', '▁of', '▁words', ',', '▁symbols', ',', '▁and', '▁meaning', 's', '.', '▁Each', '▁turn', '▁shows', '▁a', '▁bit', '▁more', '▁about', '▁the', '▁cool', '▁ways', '▁computers', '▁learn', '▁to', '▁understand', '▁our', '▁language', '.', '▁And', '▁while', '▁I', \"'\", 'm', '▁still', '▁finding', '▁my', '▁way', '▁through', '▁it', ',', '▁the', '▁journey', '’', 's', '▁been', '▁enlighten', 'ing', '▁and', ',', '▁honestly', ',', '▁a', '▁bunch', '▁of', '▁fun', '.', '▁E', 'ager', '▁to', '▁see', '▁where', '▁this', '▁learning', '▁path', '▁takes', '▁me', '▁next', '!', '\"']\n",
      "Time Taken: 0:00:00.000698 seconds\n",
      "\n",
      "XLNet Token Frequencies: {'▁Going': 1, '▁through': 3, '▁the': 3, '▁world': 1, '▁of': 3, '▁token': 1, 'ization': 1, '▁has': 1, '▁been': 2, '▁like': 1, '▁walking': 1, '▁a': 3, '▁huge': 1, '▁maze': 1, '▁made': 1, '▁words': 1, ',': 5, '▁symbols': 1, '▁and': 2, '▁meaning': 1, 's': 2, '.': 3, '▁Each': 1, '▁turn': 1, '▁shows': 1, '▁bit': 1, '▁more': 1, '▁about': 1, '▁cool': 1, '▁ways': 1, '▁computers': 1, '▁learn': 1, '▁to': 2, '▁understand': 1, '▁our': 1, '▁language': 1, '▁And': 1, '▁while': 1, '▁I': 1, \"'\": 1, 'm': 1, '▁still': 1, '▁finding': 1, '▁my': 1, '▁way': 1, '▁it': 1, '▁journey': 1, '’': 1, '▁enlighten': 1, 'ing': 1, '▁honestly': 1, '▁bunch': 1, '▁fun': 1, '▁E': 1, 'ager': 1, '▁see': 1, '▁where': 1, '▁this': 1, '▁learning': 1, '▁path': 1, '▁takes': 1, '▁me': 1, '▁next': 1, '!': 1, '\"': 1}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from transformers import BertTokenizer, XLNetTokenizer\n",
    "from datetime import datetime\n",
    "\n",
    "# NLTK Tokenization\n",
    "start_time = datetime.now()\n",
    "nltk_tokens = nltk.word_tokenize(text)\n",
    "nltk_time = datetime.now() - start_time\n",
    "\n",
    "# SpaCy Tokenization\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "start_time = datetime.now()\n",
    "spacy_tokens = [token.text for token in nlp(text)]\n",
    "spacy_time = datetime.now() - start_time\n",
    "\n",
    "# BertTokenizer Tokenization\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "start_time = datetime.now()\n",
    "bert_tokens = bert_tokenizer.tokenize(text)\n",
    "bert_time = datetime.now() - start_time\n",
    "\n",
    "# XLNetTokenizer Tokenization\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "start_time = datetime.now()\n",
    "xlnet_tokens = xlnet_tokenizer.tokenize(text)\n",
    "xlnet_time = datetime.now() - start_time\n",
    "    \n",
    "# Display tokens, time taken for each tokenizer, and token frequencies\n",
    "print(f\"NLTK Tokens: {nltk_tokens}\\nTime Taken: {nltk_time} seconds\\n\")\n",
    "show_frequencies(nltk_tokens, \"NLTK\")\n",
    "\n",
    "print(f\"SpaCy Tokens: {spacy_tokens}\\nTime Taken: {spacy_time} seconds\\n\")\n",
    "show_frequencies(spacy_tokens, \"SpaCy\")\n",
    "\n",
    "print(f\"Bert Tokens: {bert_tokens}\\nTime Taken: {bert_time} seconds\\n\")\n",
    "show_frequencies(bert_tokens, \"Bert\")\n",
    "\n",
    "print(f\"XLNet Tokens: {xlnet_tokens}\\nTime Taken: {xlnet_time} seconds\\n\")\n",
    "show_frequencies(xlnet_tokens, \"XLNet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6d042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================================\n",
    "# 4. Data augmentation and graph conversion (corrected core logic)\n",
    "# ===================================================================\n",
    "print(\"\\n--- [Step 4/5] Augmenting full dataset and converting to PyG graph objects ---\")\n",
    "data_list = []\n",
    "for _, row in tqdm(main_df.iterrows(), total=len(main_df), desc=\"Processing SMILES and augmenting\"):\n",
    "    original_smiles = row[SMILES_COL]\n",
    "    targets = [row.get(t, np.nan) for t in TARGETS]\n",
    "    y = torch.tensor(targets, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "    def process_single_smiles(smi, label):\n",
    "        graph_data = smiles_to_periodic_graph(smi)\n",
    "        if not graph_data: return None\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol:\n",
    "            fp = AllChem.GetMorganFingerprintAsBitVect(mol, MORGAN_FP_RADIUS, nBits=MORGAN_FP_DIM)\n",
    "            graph_data.morgan_fp = torch.tensor(np.array(fp), dtype=torch.float).unsqueeze(0)\n",
    "        else:\n",
    "            graph_data.morgan_fp = torch.zeros(1, MORGAN_FP_DIM, dtype=torch.float)\n",
    "        graph_data.y = label\n",
    "        return graph_data\n",
    "\n",
    "    # Process original SMILES\n",
    "    processed_original = process_single_smiles(original_smiles, y)\n",
    "    if processed_original:\n",
    "        data_list.append(processed_original)\n",
    "    \n",
    "    # Process augmented SMILES\n",
    "    augmented_smiles = augment_repeat_units(original_smiles, n_repeats=3)\n",
    "    if augmented_smiles != original_smiles:\n",
    "        processed_augmented = process_single_smiles(augmented_smiles, y)\n",
    "        if processed_augmented:\n",
    "            data_list.append(processed_augmented)\n",
    "\n",
    "print(f\"Data processing complete, generated {len(data_list)} graph objects (including original and augmented data).\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matinfo",
   "language": "python",
   "name": "matinfo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
